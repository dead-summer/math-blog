@article{dissanayake1994neural,
  title     = {Neural-network-based approximations for solving partial differential equations},
  author    = {Dissanayake, MWM Gamini and Phan-Thien, Nhan},
  journal   = {communications in Numerical Methods in Engineering},
  volume    = {10},
  number    = {3},
  pages     = {195--201},
  year      = {1994},
  publisher = {Wiley Online Library},
  doi       = {10.1002/cnm.1640100303},
  url       = {https://onlinelibrary.wiley.com/doi/10.1002/cnm.1640100303},
  abstract  = {A numerical method, based on neural-network-based functions, for solving partial differential equations is reported in the paper. Using a ‘universal approximator’ based on a neural network and point collocation, the numerical problem of solving the partial differential equation is transformed to an unconstrained minimization problem. The method is extremely easy to implement and is suitable for obtaining an approximate solution in a short period of time. The technique is illustrated with the aid of two numerical examples.}
}

@article{lagaris1998ann,
  author   = {Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
  journal  = {IEEE Transactions on Neural Networks},
  title    = {Artificial neural networks for solving ordinary and partial differential equations},
  year     = {1998},
  volume   = {9},
  number   = {5},
  pages    = {987-1000},
  keywords = {Artificial neural networks;Differential equations;Boundary conditions;Partial differential equations;Boundary value problems;Neural networks;Feedforward neural networks;Moment methods;Finite element methods;Digital signal processors},
  doi      = {10.1109/72.712178},
  pmid     = {18255782},
  url      = {http://ieeexplore.ieee.org/document/712178/},
  abstract = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the initial/boundary conditions and contains no adjustable parameters. The second part is constructed so as not to affect the initial/boundary conditions. This part involves a feedforward neural network containing adjustable parameters (the weights). Hence by construction the initial/boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ordinary differential equations (ODE), to systems of coupled ODE and also to partial differential equations (PDE). In this article, we illustrate the method by solving a variety of model problems and present comparisons with solutions obtained using the Galerkin finite element method for several cases of partial differential equations. With the advent of neuroprocessors and digital signal processors the method becomes particularly interesting due to the expected essential gains in the execution speed.}
}

@article{e2018deepritz,
  journal      = {Communications in Mathematics and Statistics},
  shortjournal = {Commun. Math. Stat.},
  issn         = {2194-6701},
  number       = {1},
  author       = {Weinan E and Bing Yu},
  title        = {The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems},
  publisher    = {Springer Science and Business Media LLC},
  language     = {en},
  year         = {2018},
  month        = {2},
  volume       = {6},
  pages        = {1--12},
  doi          = {10.1007/s40304-018-0127-z},
  url          = {http://link.springer.com/10.1007/s40304-018-0127-z},
  abstract     = {We propose a deep learning-based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz Method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems.}
}

@article{sirignano2018dgm,
  journal      = {Journal of Computational Physics},
  shortjournal = {J. Comput. Phys.},
  issn         = {0021-9991},
  author       = {Justin Sirignano and Konstantinos Spiliopoulos},
  title        = {DGM: A deep learning algorithm for solving partial differential equations},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2018},
  month        = {12},
  volume       = {375},
  pages        = {1339--1364},
  doi          = {10.1016/j.jcp.2018.08.029},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118305527},
  abstract     = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.}
}

@article{raissi2019pinn,
  journal      = {Journal of Computational Physics},
  shortjournal = {J. Comput. Phys.},
  issn         = {0021-9991},
  author       = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
  title        = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2019},
  month        = {2},
  volume       = {378},
  pages        = {686--707},
  doi          = {10.1016/j.jcp.2018.10.045},
  abstract     = {We introduce physics-informed neural networks-neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{liu2021dualdimer,
  journal      = {Neural Networks},
  shortjournal = {Neural Networks},
  issn         = {0893-6080},
  author       = {Dehao Liu and Yan Wang},
  title        = {A Dual-Dimer method for training physics-constrained neural networks with minimax architecture},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2021},
  month        = {4},
  volume       = {136},
  pages        = {112--125},
  doi          = {10.1016/j.neunet.2020.12.028},
  pmid         = {33476947},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0893608020304536},
  abstract     = {Data sparsity is a common issue to train machine learning tools such as neural networks for engineering and scientific applications, where experiments and simulations are expensive. Recently physics-constrained neural networks (PCNNs) were developed to reduce the required amount of training data. However, the weights of different losses from data and physical constraints are adjusted empirically in PCNNs. In this paper, a new physics-constrained neural network with the minimax architecture (PCNN-MM) is proposed so that the weights of different losses can be adjusted systematically. The training of the PCNN-MM is searching the high-order saddle points of the objective function. A novel saddle point search algorithm called Dual-Dimer method is developed. It is demonstrated that the Dual-Dimer method is computationally more efficient than the gradient descent ascent method for nonconvex–nonconcave functions and provides additional eigenvalue information to verify search results. A heat transfer example also shows that the convergence of PCNN-MMs is faster than that of traditional PCNNs.}
}

@article{sun2019pcnn,
  journal      = {Computer Methods in Applied Mechanics and Engineering},
  shortjournal = {Comput. Methods Appl. Mech. Eng.},
  issn         = {0045-7825},
  author       = {Luning Sun and Han Gao and Shaowu Pan and Jian-Xun Wang},
  title        = {Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data},
  publisher    = {North-Holland},
  language     = {en},
  year         = {2019},
  month        = {11},
  volume       = {361},
  doi          = {10.1016/j.cma.2019.112732},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S004578251930622X},
  abstract     = {Numerical simulations on fluid dynamics problems primarily rely on spatially or/and temporally discretization of the governing equation using polynomials into a finite-dimensional algebraic system. Due to the multi-scale nature of the physics and sensitivity from meshing a complicated geometry, such process can be computational prohibitive for most real-time applications (e.g., clinical diagnosis and surgery planning) and many-query analyses (e.g., optimization design and uncertainty quantification). Therefore, developing a cost-effective surrogate model is of great practical significance. Deep learning (DL) has shown new promises for surrogate modeling due to its capability of handling strong nonlinearity and high dimensionality. However, the off-the-shelf DL architectures, success of which heavily relies on the large amount of training data and interpolatory nature of the problem, fail to operate when the data becomes sparse. Unfortunately, data is often insufficient in most parametric fluid dynamics problems since each data point in the parameter space requires an expensive numerical simulation based on the first principle, e.g., Navier–Stokes equations. In this paper, we provide a physics-constrained DL approach for surrogate modeling of fluid flows without relying on any simulation data. Specifically, a structured deep neural network (DNN) architecture is devised to enforce the initial and boundary conditions, and the governing partial differential equations (i.e., Navier–Stokes equations) are incorporated into the loss of the DNN to drive the training. Numerical experiments are conducted on a number of internal flows relevant to hemodynamics applications, and the forward propagation of uncertainties in fluid properties and domain geometry is studied as well. The results show excellent agreement on the flow field and forward-propagated uncertainties between the DL surrogate approximations and the first-principle numerical simulations.}
}

@article{zhu2019pcnn,
  journal      = {Journal of Computational Physics},
  shortjournal = {J. Comput. Phys.},
  issn         = {0021-9991},
  author       = {Yinhao Zhu and Nicholas Zabaras and Phaedon-Stelios Koutsourelakis and Paris Perdikaris},
  title        = {Physics-Constrained Deep Learning for High-dimensional Surrogate Modeling and Uncertainty Quantification without Labeled Data},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2019},
  month        = {01},
  volume       = {394},
  pages        = {56--81},
  doi          = {10.1016/j.jcp.2019.05.024},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0021999119303559},
  abstract     = {Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.}
}

@article{kharazmi2020hpvpinn,
  journal      = {Computer Methods in Applied Mechanics and Engineering},
  shortjournal = {Comput. Methods Appl. Mech. Eng.},
  issn         = {0045-7825},
  author       = {Ehsan Kharazmi and Zhongqiang Zhang and George E.M. Karniadakis},
  title        = {hp-VPINNs: Variational Physics-Informed Neural Networks With Domain Decomposition},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2020},
  month        = {03},
  volume       = {abs/2003.05385},
  doi          = {10.1016/j.cma.2020.113547},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0045782520307325},
  abstract     = {We formulate a general framework for hp-variational physics-informed neural networks (hp-VPINNs) based on the nonlinear approximation of shallow and deep neural networks and hp-refinement via domain decomposition and projection onto the space of high-order polynomials. The trial space is the space of neural network, which is defined globally over the entire computational domain, while the test space contains piecewise polynomials. Specifically in this study, the hp-refinement corresponds to a global approximation with a local learning algorithm that can efficiently localize the network parameter optimization. We demonstrate the advantages of hp-VPINNs in both accuracy and training cost for several numerical examples of function approximation and in solving differential equations.}
}

@article{jagtap2020cpinn,
  journal      = {Computer Methods in Applied Mechanics and Engineering},
  shortjournal = {Comput. Methods Appl. Mech. Eng.},
  issn         = {0045-7825},
  author       = {Ameya D. Jagtap and Ehsan Kharazmi and George Em Karniadakis},
  title        = {Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2020},
  month        = {06},
  volume       = {365},
  doi          = {10.1016/j.cma.2020.113028},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0045782520302127},
  abstract     = {We propose a conservative physics-informed neural network (cPINN) on discrete domains for nonlinear conservation laws. Here, the term discrete domain represents the discrete sub-domains obtained after division of the computational domain, where PINN is applied and the conservation property of cPINN is obtained by enforcing the flux continuity in the strong form along the sub-domain interfaces. In case of hyperbolic conservation laws, the convective flux contributes at the interfaces, whereas in case of viscous conservation laws, both convective and diffusive fluxes contribute. Apart from the flux continuity condition, an average solution (given by two different neural networks) is also enforced at the common interface between two sub-domains. One can also employ a deep neural network in the domain, where the solution may have complex structure, whereas a shallow neural network can be used in the sub-domains with relatively simple and smooth solutions. Another advantage of the proposed method is the additional freedom it gives in terms of the choice of optimization algorithm and the various training parameters like residual points, activation function, width and depth of the network etc. Various forms of errors involved in cPINN such as optimization, generalization and approximation errors and their sources are discussed briefly. In cPINN, locally adaptive activation functions are used, hence training the model faster compared to its fixed counterparts. Both, forward and inverse problems are solved using the proposed method. Various test cases ranging from scalar nonlinear conservation laws like Burgers, Korteweg–de Vries (KdV) equations to systems of conservation laws, like compressible Euler equations are solved. The lid-driven cavity test case governed by incompressible Navier–Stokes equation is also solved and the results are compared against a benchmark solution. The proposed method enjoys the property of domain decomposition with separate neural networks in each sub-domain, and it efficiently lends itself to parallelized computation, where each sub-domain can be assigned to a different computational node.}
}

@article{dwivedi2019pielm,
  journal      = {Neurocomputing},
  shortjournal = {Neurocomputing},
  issn         = {0925-2312},
  author       = {Vikas Dwivedi and Balaji Srinivasan},
  title        = {Physics Informed Extreme Learning Machine (PIELM) - A rapid method for the numerical solution of partial differential equations},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2019},
  month        = {07},
  volume       = {abs/1907.03507},
  pages        = {96--118},
  doi          = {10.1016/j.neucom.2019.12.099},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219318144},
  abstract     = {There has been rapid progress recently on the application of deep networks to the solution of partial differential equations, collectively labeled as Physics Informed Neural Networks (PINNs). In this paper, We develop Physics Informed Extreme Learning Machine (PIELM), a rapid version of PINNs which can be applied to stationary and time-dependent linear partial differential equations. We demonstrate that PIELM matches or exceeds the accuracy of PINNs on a range of problems. We also discuss the limitations of neural network-based approaches, including our PIELM, in the solution of PDEs on large domains and suggest an extension, a distributed version of our algorithm -- DPIELM. We show that DPIELM produces excellent results comparable to conventional numerical techniques in the solution of time-dependent problems. Collectively, this work contributes towards making the use of neural networks in the solution of partial differential equations in complex domains as a competitive alternative to conventional discretization techniques.}
}

@article{amuthan2021spinn,
  journal      = {Journal of Computational Physics},
  shortjournal = {J. Comput. Phys.},
  issn         = {0021-9991},
  author       = {Amuthan A. Ramabathiran and Prabhu Ramachandran},
  title        = {SPINN: Sparse, Physics-based, and partially Interpretable Neural Networks for PDEs},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2021},
  month        = {11},
  volume       = {445},
  doi          = {10.1016/j.jcp.2021.110600},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0021999121004952},
  abstract     = {We introduce a class of Sparse, Physics-based, and partially Interpretable Neural Networks (SPINN) for solving ordinary and partial differential equations (PDEs). By reinterpreting a traditional meshless representation of solutions of PDEs we develop a class of sparse neural network architectures that are partially interpretable. The SPINN model we propose here serves as a seamless bridge between two extreme modeling tools for PDEs, namely dense neural network based methods like Physics Informed Neural Networks (PINNs) and traditional mesh-free numerical methods, thereby providing a novel means to develop a new class of hybrid algorithms that build on the best of both these viewpoints. A unique feature of the SPINN model that distinguishes it from other neural network based approximations proposed earlier is that it is (i) interpretable, in a particular sense made precise in the work, and (ii) sparse in the sense that it has much fewer connections than typical dense neural networks used for PDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is able to handle discontinuities in the solutions. In addition, we demonstrate that Fourier series representations can also be expressed as a special class of SPINN and propose generalized neural network analogues of Fourier representations. We illustrate the utility of the proposed method with a variety of examples involving ordinary differential equations, elliptic, parabolic, hyperbolic and nonlinear partial differential equations, and an example in fluid dynamics.}
}

@article{lu2021deeponet,
  journal      = {Nature Machine Intelligence},
  shortjournal = {Nat. Mach. Intell.},
  issn         = {2522-5839},
  number       = {3},
  author       = {Lu Lu and Pengzhan Jin and Guofei Pang and Zhongqiang Zhang and George Em Karniadakis and Lu Lu and Pengzhan Jin and Guofei Pang and Zhongqiang Zhang and George Em Karniadakis},
  title        = {Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
  publisher    = {Springer Science and Business Media LLC},
  language     = {en},
  year         = {2021},
  month        = {3},
  volume       = {3},
  pages        = {218--229},
  doi          = {10.1038/s42256-021-00302-5},
  url          = {https://www.nature.com/articles/s42256-021-00302-5},
  abstract     = {It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications.}
}

@article{gao2020phygeonet,
  journal      = {Journal of Computational Physics},
  shortjournal = {J. Comput. Phys.},
  issn         = {0021-9991},
  author       = {Han Gao and Luning Sun and Jian-Xun Wang},
  title        = {PhyGeoNet: Physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state PDEs on irregular domain},
  publisher    = {Academic Press},
  language     = {en},
  year         = {2020},
  month        = {12},
  volume       = {428},
  doi          = {10.1016/j.jcp.2020.110079},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0021999120308536},
  abstract     = {Recently, the advent of deep learning has spurred interest in the development of physics-informed neural networks (PINN) for efficiently solving partial differential equations (PDEs), particularly in a parametric setting. Among all different classes of deep neural networks, the convolutional neural network (CNN) has attracted increasing attention in the scientific machine learning community, since the parameter-sharing feature in CNN enables efficient learning for problems with large-scale spatiotemporal fields. However, one of the biggest challenges is that CNN only can handle regular geometries with image-like format (i.e., rectangular domains with uniform grids). In this paper, we propose a novel physics-constrained CNN learning architecture, aiming to learn solutions of parametric PDEs on irregular domains without any labeled data. In order to leverage powerful classic CNN backbones, elliptic coordinate mapping is introduced to enable coordinate transforms between the irregular physical domain and regular reference domain. The proposed method has been assessed by solving a number of steady-state PDEs on irregular domains, including heat equations, Navier-Stokes equations, and Poisson equations with parameterized boundary conditions, varying geometries, and spatially-varying source fields. Moreover, the proposed method has also been compared against the state-of-the-art PINN with fully-connected neural network (FC-NN) formulation. The numerical results demonstrate the effectiveness of the proposed approach and exhibit notable superiority over the FC-NN based PINN in terms of efficiency and accuracy.}
}

@article{wang2021tgae,
  journal      = {Computer Methods in Applied Mechanics and Engineering},
  shortjournal = {Comput. Methods Appl. Mech. Eng.},
  issn         = {0045-7825},
  author       = {Nanzhe Wang and Haibin Chang and Dongxiao Zhang},
  title        = {Theory-guided Auto-Encoder for surrogate construction and inverse modeling},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2021},
  month        = {11},
  volume       = {385},
  doi          = {10.1016/j.cma.2021.114037},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0045782521003686},
  abstract     = {A Theory-guided Auto-Encoder (TgAE) framework is proposed for surrogate construction, and is further used for uncertainty quantification and inverse modeling tasks. The framework is built based on the Auto-Encoder (or Encoder–Decoder) architecture of the convolutional neural network (CNN) via a theory-guided training process. In order to incorporate physical constraints for achieving theory-guided training, the governing equations of the studied problems can be discretized by the finite difference scheme, and then be embedded into the training of the CNN. The residual of the discretized governing equations, as well as the data mismatch, constitute the loss function of the TgAE. The trained TgAE can be utilized to construct a surrogate that approximates the relationship between the model parameters and model responses with limited labeled data. Several subsurface flow cases are designed to test the performance of the TgAE. The results demonstrate that satisfactory accuracy for surrogate modeling and higher efficiency for uncertainty quantification tasks can be achieved with the TgAE. The TgAE also shows good extrapolation ability for cases with different correlation lengths and variances. Furthermore, inverse modeling tasks are also implemented with the TgAE surrogate, and satisfactory results are obtained.}
}

@article{viana2021pirnn,
  journal      = {Computers & Structures},
  shortjournal = {Comput. Struct.},
  issn         = {0045-7949},
  author       = {Felipe A.C. Viana and Renato G. Nascimento and Arinan Dourado and Yigit A. Yucesan},
  title        = {Estimating model inadequacy in ordinary differential equations with physics-informed neural networks},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2021},
  month        = {3},
  volume       = {245},
  doi          = {10.1016/j.compstruc.2020.106458},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0045794920302613},
  abstract     = {A number of physical systems can be described by ordinary differential equations. When physics is well understood, the time dependent responses are easily obtained numerically. The particular numerical method used for integration depends on the application. Unfortunately, when physics is not fully understood, the discrepancies between predictions and observed responses can be large and unacceptable. In this paper, we propose an approach that uses observed data to estimate the missing physics in the original model (i.e., model-form uncertainty). In our approach, we first design recurrent neural networks to perform numerical integration of the ordinary differential equations. Then, we implement the recurrent neural network as a directed graph. This way, the nodes in the graph represent the physics-informed kernels found in the ordinary differential equations. We quantify the missing physics by carefully introducing data-driven in the directed graph. This allows us to estimate the missing physics (discrepancy term) even for hidden nodes of the graph. We studied the performance of our proposed approach with the aid of three case studies (fatigue crack growth, corrosion-fatigue crack growth, and bearing fatigue) and state-of-the-art machine learning software packages. Our results demonstrate the ability to perform estimation of discrepancy, reducing gap between predictions and observations, at reasonable computational cost.}
}

@article{zhang2020pimlmlstm,
  journal      = {Computer Methods in Applied Mechanics and Engineering},
  shortjournal = {Comput. Methods Appl. Mech. Eng.},
  issn         = {0045-7825},
  author       = {Ruiyang Zhang and Yang Liu and Hao Sun},
  title        = {Physics-informed multi-LSTM networks for metamodeling of nonlinear structures},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2020},
  month        = {9},
  volume       = {369},
  doi          = {10.1016/j.cma.2020.113226},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0045782520304114},
  abstract     = {This paper introduces an innovative physics-informed deep learning framework for metamodeling of nonlinear structural systems with scarce data. The basic concept is to incorporate available, yet incomplete, physics knowledge (e.g., laws of physics, scientific principles) into deep long short-term memory (LSTM) networks, which constrains and boosts the learning within a feasible solution space. The physics constraints are embedded in the loss function to enforce the model training which can accurately capture latent system nonlinearity even with very limited available training datasets. Specifically for dynamic structures, physical laws of equation of motion, state dependency and hysteretic constitutive relationship are considered to construct the physics loss. In particular, two physics-informed multi-LSTM network architectures are proposed for structural metamodeling. The satisfactory performance of the proposed framework is successfully demonstrated through two illustrative examples (e.g., nonlinear structures subjected to ground motion excitation). It turns out that the embedded physics can alleviate overfitting issues, reduce the need of big training datasets, and improve the robustness of the trained model for more reliable prediction with extrapolation ability. As a result, the physics-informed deep learning paradigm outperforms classical non-physics-guided data-driven neural networks.}
}

@article{yang2021bpinn,
  journal      = {Journal of Computational Physics},
  shortjournal = {J. Comput. Phys.},
  issn         = {0021-9991},
  author       = {Liu Yang and Xuhui Meng and George Em Karniadakis},
  title        = {B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2021},
  month        = {1},
  volume       = {425},
  doi          = {10.1016/j.jcp.2020.109913},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0021999120306872},
  abstract     = {We propose a Bayesian physics-informed neural network (B-PINN) to solve both forward and inverse nonlinear problems described by partial differential equations (PDEs) and noisy data. In this Bayesian framework, the Bayesian neural network (BNN) combined with a PINN for PDEs serves as the prior while the Hamiltonian Monte Carlo (HMC) or the variational inference (VI) could serve as an estimator of the posterior. B-PINNs make use of both physical laws and scattered noisy measurements to provide predictions and quantify the aleatoric uncertainty arising from the noisy data in the Bayesian framework. Compared with PINNs, in addition to uncertainty quantification, B-PINNs obtain more accurate predictions in scenarios with large noise due to their capability of avoiding overfitting. We conduct a systematic comparison between the two different approaches for the B-PINNs posterior estimation (i.e., HMC or VI), along with dropout used for quantifying uncertainty in deep neural networks. Our experiments show that HMC is more suitable than VI with mean field Gaussian approximation for the B-PINNs posterior estimation, while dropout employed in PINNs can hardly provide accurate predictions with reasonable uncertainty. Finally, we replace the BNN in the prior with a truncated Karhunen-Loève (KL) expansion combined with HMC or a deep normalizing flow (DNF) model as posterior estimators. The KL is as accurate as BNN and much faster but this framework cannot be easily extended to high-dimensional problems unlike the BNN based framework.}
}

@article{liu2020pigan,
  journal      = {SIAM Journal on Scientific Computing},
  shortjournal = {SIAM J. Sci. Comput.},
  issn         = {1064-8275},
  number       = {1},
  author       = {Yang, Liu and Zhang, Dongkun and Karniadakis, George Em},
  title        = {Physics-Informed Generative Adversarial Networks for Stochastic Differential Equations},
  publisher    = {Society for Industrial & Applied Mathematics (SIAM)},
  language     = {en},
  year         = {2020},
  month        = {1},
  volume       = {42},
  pages        = {A292--A317},
  doi          = {10.1137/18M1225409},
  url          = {https://epubs.siam.org/doi/10.1137/18M1225409},
  urldate      = {2025-12-23}
}

@inbook{stiller2020gatedpinn,
  booktitle = {Driving Scientific and Engineering Discoveries Through the Convergence of HPC, Big Data and AI},
  isbn      = {1865-0929},
  author    = {Patrick Stiller and Friedrich Bethke and Maximilian Böhme and Richard Pausch and Sunna Torge and Alexander Debus and Jan Vorberger and Michael Bussmann and Nico Hoffmann},
  title     = {Large-Scale Neural Solvers for Partial Differential Equations},
  publisher = {Springer International Publishing},
  language  = {en},
  year      = {2020},
  pages     = {20--34},
  doi       = {10.1007/978-3-030-63393-6_2},
  url       = {https://link.springer.com/10.1007/978-3-030-63393-6_2},
  address   = {Cham},
  urldate   = {2025-12-23}
}

@article{cai2021deepmmnet,
  journal      = {Journal of Computational Physics},
  shortjournal = {J. Comput. Phys.},
  issn         = {0021-9991},
  author       = {Shengze Cai and Zhicheng Wang and Lu Lu and Tamer A. Zaki and George Em Karniadakis},
  title        = {DeepM&amp;Mnet: Inferring the electroconvection multiphysics fields based on operator approximation by neural networks},
  publisher    = {Elsevier BV},
  language     = {en},
  year         = {2021},
  month        = {7},
  volume       = {436},
  doi          = {10.1016/j.jcp.2021.110296},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0021999121001911},
  urldate      = {2025-12-23}
}